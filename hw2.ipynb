{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911e5fc1",
   "metadata": {},
   "source": [
    "### Homework 2: Uncertainty and Vision\n",
    "CS-229 Spring 2023\n",
    "\n",
    "The goal of this assignment is to get familiar with training a computer vision task (Segmentation) with PyTorch, and to measure confidence calibration in your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549dab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b36ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToIntTensor(transforms.ToTensor):\n",
    "    \"\"\"A custom transform that replaces \"ToTensor\". ToTensor always converts to a \n",
    "    a float in range [0,1]. This one converts to an integer, which can represent\n",
    "    our class labels per pixel in an image segmentation problem\"\"\"\n",
    "    def __call__(self, pic):\n",
    "        tensor = super().__call__(pic)\n",
    "        tensor = (tensor * 255).to(torch.int64)\n",
    "        return tensor\n",
    "\n",
    "def get_voc_dataloader(batch_size=4):\n",
    "    \"\"\"Get the VOC 2007 segmentation dataset and return PyTorch \n",
    "    dataloaders for both training and validation. \n",
    "    \"\"\"\n",
    "    # TODO: 2 points. Define image transforms for both the input image AND the \"label\" image.\n",
    "    # Remember, the labels are one category (integer) per pixel. So, while the image_transform\n",
    "    # normalize the values to have mean zero and unit standard deviation, you shouldn't \n",
    "    # do that with the labels, which have to be integers. I provided a \"ToIntTensor\" transform\n",
    "    # above to use for the label transform, instead of ToTensor which always turns images to floats. \n",
    "    # Also, we want to resize/crop images to be all the same size, a power of 2, and \n",
    "    # we should transform the label and image in the same way when changing size.\n",
    "    # The size of images will drastically impact memory usage - I suggest targeting 128 x 128 \n",
    "    # or even 64 x 64 if memory constraints are an issue. \n",
    "    image_transforms = # TODO\n",
    "    label_transforms = # TODO\n",
    "\n",
    "    # This downloads the data automatically and creates a \"dataset\" object that applies the transforms\n",
    "    data_dir = \"~/Downloads/\"  # TODO: Specify path to save data\n",
    "    train_dataset = datasets.VOCSegmentation(data_dir, year='2007', image_set='train', download=True, transform=image_transforms, target_transform=label_transforms)\n",
    "    val_dataset = datasets.VOCSegmentation(data_dir, year='2007', image_set='val', download=True, transform=image_transforms, target_transform=label_transforms)\n",
    "\n",
    "    # Create data loaders for the datasets - necessary for efficient training\n",
    "    train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dl, val_dl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ee22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dl, val_dl, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train one epoch of model with optimizer, using data from train_dl.\n",
    "    Do training on \"device\". \n",
    "    Return the train and validation loss and validation accuracy.\n",
    "    \"\"\"\n",
    "    # We'll use the cross entropy loss. There's a nice feature that it\n",
    "    # allows you to \"ignore_index\". In this case index 255 is the mask to ignore\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)  # recommend to use in constructing loss\n",
    "\n",
    "    # TODO: Train (3 points)\n",
    "    # Iterate over the train dataloader\n",
    "    # Put data batch on same device as model\n",
    "    # \"Forward pass\" - run data through model, and use output to calculate loss. \n",
    "    # \"backward pass\"\n",
    "    # Remember to keep track of training loss during loop. \n",
    "        \n",
    "    # TODO: Validation loss and accuracy (2 points)\n",
    "    # estimate the loss on the validation dataset\n",
    "    # The network should be in \"eval\" mode (remember to go back to train mode for training)\n",
    "    # Turn off grad tracking for speed \n",
    "    # Accuracy on validation datais very helpful to output - \n",
    "    # 69 percent of pixels are \"background\" - we hope to get better accuracy than that!\n",
    "\n",
    "    return train_loss, val_loss, accuracy \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badac95",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b3c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/gregv/Downloads/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting /home/gregv/Downloads/VOCtrainval_06-Nov-2007.tar to /home/gregv/Downloads/\n",
      "Using downloaded and verified file: /home/gregv/Downloads/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting /home/gregv/Downloads/VOCtrainval_06-Nov-2007.tar to /home/gregv/Downloads/\n",
      "Epoch 1: Train loss: 2.243 | Val loss: 2.400 | Acc: 0.509\n",
      "Epoch 2: Train loss: 1.064 | Val loss: 1.143 | Acc: 0.737\n",
      "Epoch 3: Train loss: 0.825 | Val loss: 1.243 | Acc: 0.709\n",
      "Epoch 4: Train loss: 0.694 | Val loss: 0.860 | Acc: 0.770\n",
      "Epoch 5: Train loss: 0.525 | Val loss: 0.815 | Acc: 0.769\n",
      "Epoch 6: Train loss: 0.421 | Val loss: 0.920 | Acc: 0.760\n",
      "Epoch 7: Train loss: 0.389 | Val loss: 0.794 | Acc: 0.786\n",
      "Epoch 8: Train loss: 0.334 | Val loss: 0.788 | Acc: 0.788\n",
      "Epoch 9: Train loss: 0.297 | Val loss: 0.834 | Acc: 0.782\n",
      "Epoch 10: Train loss: 0.286 | Val loss: 0.768 | Acc: 0.792\n",
      "Epoch 11: Train loss: 0.261 | Val loss: 0.691 | Acc: 0.805\n",
      "Epoch 12: Train loss: 0.239 | Val loss: 0.674 | Acc: 0.807\n",
      "Epoch 13: Train loss: 0.247 | Val loss: 0.709 | Acc: 0.803\n",
      "Epoch 14: Train loss: 0.216 | Val loss: 0.720 | Acc: 0.803\n",
      "Epoch 15: Train loss: 0.210 | Val loss: 0.694 | Acc: 0.807\n",
      "Epoch 16: Train loss: 0.217 | Val loss: 0.681 | Acc: 0.810\n",
      "Epoch 17: Train loss: 0.200 | Val loss: 0.679 | Acc: 0.805\n",
      "Epoch 18: Train loss: 0.184 | Val loss: 0.688 | Acc: 0.806\n",
      "Epoch 19: Train loss: 0.185 | Val loss: 0.722 | Acc: 0.805\n",
      "Epoch 20: Train loss: 0.186 | Val loss: 0.696 | Acc: 0.810\n",
      "Epoch 21: Train loss: 0.178 | Val loss: 0.655 | Acc: 0.813\n",
      "Epoch 22: Train loss: 0.172 | Val loss: 0.673 | Acc: 0.812\n",
      "Epoch 23: Train loss: 0.168 | Val loss: 0.684 | Acc: 0.811\n",
      "Epoch 24: Train loss: 0.179 | Val loss: 0.665 | Acc: 0.812\n",
      "Epoch 25: Train loss: 0.157 | Val loss: 0.626 | Acc: 0.817\n",
      "Epoch 26: Train loss: 0.158 | Val loss: 0.637 | Acc: 0.817\n",
      "Epoch 27: Train loss: 0.161 | Val loss: 0.675 | Acc: 0.815\n",
      "Epoch 28: Train loss: 0.153 | Val loss: 0.673 | Acc: 0.813\n",
      "Epoch 29: Train loss: 0.136 | Val loss: 0.684 | Acc: 0.811\n",
      "Epoch 30: Train loss: 0.148 | Val loss: 0.685 | Acc: 0.811\n",
      "Epoch 31: Train loss: 0.132 | Val loss: 0.639 | Acc: 0.816\n",
      "Epoch 32: Train loss: 0.136 | Val loss: 0.644 | Acc: 0.818\n",
      "Epoch 33: Train loss: 0.139 | Val loss: 0.658 | Acc: 0.815\n",
      "Epoch 34: Train loss: 0.144 | Val loss: 0.681 | Acc: 0.813\n",
      "Epoch 35: Train loss: 0.136 | Val loss: 0.674 | Acc: 0.814\n",
      "Epoch 36: Train loss: 0.121 | Val loss: 0.711 | Acc: 0.811\n",
      "Epoch 37: Train loss: 0.125 | Val loss: 0.687 | Acc: 0.813\n",
      "Epoch 38: Train loss: 0.120 | Val loss: 0.643 | Acc: 0.819\n",
      "Epoch 39: Train loss: 0.117 | Val loss: 0.636 | Acc: 0.819\n",
      "Epoch 40: Train loss: 0.119 | Val loss: 0.676 | Acc: 0.815\n",
      "Epoch 41: Train loss: 0.116 | Val loss: 0.707 | Acc: 0.812\n",
      "Epoch 42: Train loss: 0.107 | Val loss: 0.691 | Acc: 0.812\n",
      "Epoch 43: Train loss: 0.111 | Val loss: 0.672 | Acc: 0.815\n",
      "Epoch 44: Train loss: 0.108 | Val loss: 0.668 | Acc: 0.816\n",
      "Epoch 45: Train loss: 0.109 | Val loss: 0.705 | Acc: 0.815\n",
      "Epoch 46: Train loss: 0.106 | Val loss: 0.691 | Acc: 0.816\n",
      "Epoch 47: Train loss: 0.101 | Val loss: 0.681 | Acc: 0.816\n",
      "Epoch 48: Train loss: 0.104 | Val loss: 0.691 | Acc: 0.814\n",
      "Epoch 49: Train loss: 0.102 | Val loss: 0.689 | Acc: 0.815\n",
      "Epoch 50: Train loss: 0.096 | Val loss: 0.663 | Acc: 0.818\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
    "\n",
    "# Define some hyperparameters\n",
    "batch_size = 64  # Adjust batch size to make maximal use of GPU without running out of memory \n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "n_class = 21  # The class labels are 0...20. The label \"255\" is interpreted as a \"mask\" meant to be ignored\n",
    "\n",
    "# Load model and data\n",
    "model = fcn_resnet50(n_class=n_class).to(device)\n",
    "train_dl, val_dl = get_voc_dataloader(batch_size=batch_size)\n",
    "\n",
    "# Training loop\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    train_loss, val_loss, accuracy = train_epoch(model, train_dl, val_dl, optimizer, device)\n",
    "    \n",
    "    # Print the loss, and store for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('Epoch %d: Train loss: %.3f | Val loss: %.3f | Acc: %.3f' % (epoch+1, train_loss, val_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351cf08",
   "metadata": {},
   "source": [
    "## Post training visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and test loss plot\n",
    "# TODO: Print out an example image, true segmentation, and predicted segmentation\n",
    "# 1 point\n",
    "# Use a colormap to get good visualizations of the segmentation classes\n",
    "# cmap = plt.cm.get_cmap('tab20', n_class + 1)  # tab20 is a colormap with 20 distinct colors\n",
    "# cmap(numpy class array) outputs something with nice colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd28f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use the outputs of my code for the confidence calibration plot and ECE\n",
    "# Here I run the model on all points in the validation set. \n",
    "# I collect predictions on all pixels, excluding masks, and flatten them. \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_acc = []\n",
    "    all_conf = []\n",
    "    for i, (inputs, labels) in enumerate(val_dl):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)['out']\n",
    "        labels = labels.squeeze(1).flatten(start_dim=1)  # batch, h, w  - integer values 0..20 or 255 for mask\n",
    "        predicted_label = outputs.argmax(axis=1).flatten(start_dim=1)  # batch, h, w, integer 0...20  \n",
    "        probs = outputs.softmax(axis=1)  # batch, n_class, h, w\n",
    "        confidence = probs.max(axis=1).values.flatten(start_dim=1)  # Confidence in predicted label\n",
    "        accuracy = (predicted_label == labels)\n",
    "        accuracy_filter = accuracy[labels < 255]\n",
    "        confidence_filter = confidence[labels < 255]\n",
    "        all_acc.append(accuracy_filter)\n",
    "        all_conf.append(confidence_filter)\n",
    "        \n",
    "all_acc = torch.cat(all_acc).cpu().numpy()  # accuracy to predict pixel class across all pixels and images, excluding masks\n",
    "all_conf = torch.cat(all_conf).cpu().numpy()  # confidence of prediction for each pixel and image, excluding masks\n",
    "        \n",
    "# Get the average confidence and accuracy for points within different confidence ranges\n",
    "bins = 10\n",
    "bin_boundaries = np.linspace(0, 1, bins + 1)\n",
    "bin_lowers = bin_boundaries[:-1]\n",
    "bin_uppers = bin_boundaries[1:]\n",
    "bin_centers = 0.5*(bin_lowers+bin_uppers)\n",
    "bin_acc = np.zeros(bins)  # Store accuracy within each bin\n",
    "bin_conf = np.zeros(bins)  # Store confidence within each bin\n",
    "bin_frac = np.zeros(bins)  # Store the fraction of data in included in each bin\n",
    "for i in range(bins):\n",
    "    in_bin = np.logical_and(all_conf >= bin_lowers[i], all_conf < bin_uppers[i])\n",
    "    bin_frac[i] = np.sum(in_bin) / len(all_conf)  # fraction of points in bin\n",
    "    if bin_frac[i] > 0.:\n",
    "        bin_acc[i] = all_acc[in_bin].mean()  # average accuracy in this bin\n",
    "        bin_conf[i] = all_conf[in_bin].mean()  # average confidence in this bin\n",
    "    else:\n",
    "        bin_acc[i], bin_conf[i] = 0, 0  # If no points are in this bin, they don't contribute to ECE anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fce827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot confidence calibration curve and calculate expected calibration error (1 point), using \n",
    "ece = # TODO\n",
    "print(\"ECE: \", ece)\n",
    "\n",
    "# TODO: confidence versus accuracy bar chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
